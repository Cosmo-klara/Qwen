{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c933f16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export VLLM_USE_MODELSCOPE=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38406013",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=1,7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d51beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def rgba_to_rgb(\n",
    "    image: Image.Image,\n",
    "    background_color: tuple[int, int, int] | list[int] = (255, 255, 255),\n",
    ") -> Image.Image:\n",
    "    \"\"\"Convert an RGBA image to RGB with filled background color.\"\"\"\n",
    "    assert image.mode == \"RGBA\"\n",
    "    converted = Image.new(\"RGB\", image.size, background_color)\n",
    "    converted.paste(image, mask=image.split()[3])  # 3 is the alpha channel\n",
    "    return converted\n",
    "\n",
    "def convert_image_mode(image: Image.Image, to_mode: str):\n",
    "    if image.mode == to_mode:\n",
    "        return image\n",
    "    elif image.mode == \"RGBA\" and to_mode == \"RGB\":\n",
    "        return rgba_to_rgb(image)\n",
    "    else:\n",
    "        return image.convert(to_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e5e2a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-04 00:23:25 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h]\n",
      "                             [--query-type {mixed_modalities,use_audio_in_video,multi_audios}]\n",
      "                             [--seed SEED]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/run/user/1001/jupyter/runtime/kernel-v35ecd5d92cebec070b5b6a54d71eda56800f13349.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/conda/vllm-qwen/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# SPDX-License-Identifier: Apache-2.0\n",
    "# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n",
    "\"\"\"\n",
    "This example shows how to use vLLM for running offline inference\n",
    "with the correct prompt format on Qwen2.5-Omni (thinker only).\n",
    "\"\"\"\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.assets.audio import AudioAsset\n",
    "from vllm.assets.image import ImageAsset\n",
    "from vllm.assets.video import VideoAsset\n",
    "from vllm.utils import FlexibleArgumentParser\n",
    "\n",
    "\n",
    "class QueryResult(NamedTuple):\n",
    "    inputs: dict\n",
    "    limit_mm_per_prompt: dict[str, int]\n",
    "\n",
    "\n",
    "# NOTE: The default `max_num_seqs` and `max_model_len` may result in OOM on\n",
    "# lower-end GPUs.\n",
    "# Unless specified, these settings have been tested to work on a single L4.\n",
    "\n",
    "default_system = (\n",
    "    \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba \"\n",
    "    \"Group, capable of perceiving auditory and visual inputs, as well as \"\n",
    "    \"generating text and speech.\"\n",
    ")\n",
    "\n",
    "\n",
    "def get_mixed_modalities_query() -> QueryResult:\n",
    "    question = (\n",
    "        \"What is recited in the audio? \"\n",
    "        \"What is the content of this image? Why is this video funny?\"\n",
    "    )\n",
    "    prompt = (\n",
    "        f\"<|im_start|>system\\n{default_system}<|im_end|>\\n\"\n",
    "        \"<|im_start|>user\\n<|audio_bos|><|AUDIO|><|audio_eos|>\"\n",
    "        \"<|vision_bos|><|IMAGE|><|vision_eos|>\"\n",
    "        \"<|vision_bos|><|VIDEO|><|vision_eos|>\"\n",
    "        f\"{question}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>assistant\\n\"\n",
    "    )\n",
    "    return QueryResult(\n",
    "        inputs={\n",
    "            \"prompt\": prompt,\n",
    "            \"multi_modal_data\": {\n",
    "                \"audio\": AudioAsset(\"mary_had_lamb\").audio_and_sample_rate,\n",
    "                \"image\": convert_image_mode(\n",
    "                    ImageAsset(\"cherry_blossom\").pil_image, \"RGB\"\n",
    "                ),\n",
    "                \"video\": VideoAsset(name=\"baby_reading\", num_frames=16).np_ndarrays,\n",
    "            },\n",
    "        },\n",
    "        limit_mm_per_prompt={\"audio\": 1, \"image\": 1, \"video\": 1},\n",
    "    )\n",
    "\n",
    "\n",
    "def get_use_audio_in_video_query() -> QueryResult:\n",
    "    question = (\n",
    "        \"Describe the content of the video, then convert what the baby say into text.\"\n",
    "    )\n",
    "    prompt = (\n",
    "        f\"<|im_start|>system\\n{default_system}<|im_end|>\\n\"\n",
    "        \"<|im_start|>user\\n<|vision_bos|><|VIDEO|><|vision_eos|>\"\n",
    "        f\"{question}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>assistant\\n\"\n",
    "    )\n",
    "    asset = VideoAsset(name=\"baby_reading\", num_frames=16)\n",
    "    audio = asset.get_audio(sampling_rate=16000)\n",
    "\n",
    "    return QueryResult(\n",
    "        inputs={\n",
    "            \"prompt\": prompt,\n",
    "            \"multi_modal_data\": {\n",
    "                \"video\": asset.np_ndarrays,\n",
    "                \"audio\": audio,\n",
    "            },\n",
    "            \"mm_processor_kwargs\": {\n",
    "                \"use_audio_in_video\": True,\n",
    "            },\n",
    "        },\n",
    "        limit_mm_per_prompt={\"audio\": 1, \"video\": 1},\n",
    "    )\n",
    "\n",
    "\n",
    "def get_multi_audios_query() -> QueryResult:\n",
    "    question = \"Are these two audio clips the same?\"\n",
    "    prompt = (\n",
    "        f\"<|im_start|>system\\n{default_system}<|im_end|>\\n\"\n",
    "        \"<|im_start|>user\\n<|audio_bos|><|AUDIO|><|audio_eos|>\"\n",
    "        \"<|audio_bos|><|AUDIO|><|audio_eos|>\"\n",
    "        f\"{question}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>assistant\\n\"\n",
    "    )\n",
    "    return QueryResult(\n",
    "        inputs={\n",
    "            \"prompt\": prompt,\n",
    "            \"multi_modal_data\": {\n",
    "                \"audio\": [\n",
    "                    AudioAsset(\"winning_call\").audio_and_sample_rate,\n",
    "                    AudioAsset(\"mary_had_lamb\").audio_and_sample_rate,\n",
    "                ],\n",
    "            },\n",
    "        },\n",
    "        limit_mm_per_prompt={\n",
    "            \"audio\": 2,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "query_map = {\n",
    "    \"mixed_modalities\": get_mixed_modalities_query,\n",
    "    \"use_audio_in_video\": get_use_audio_in_video_query,\n",
    "    \"multi_audios\": get_multi_audios_query,\n",
    "}\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    # model_name = \"Qwen/Qwen2.5-Omni-7B\"\n",
    "    # 使用本地模型\n",
    "    model_name = \"../cache/modelscope/Qwen/Qwen2.5-Omni-3BQwen/Qwen2.5-Omni-3B\"\n",
    "\n",
    "    query_result = query_map[args.query_type]()\n",
    "\n",
    "    llm = LLM(\n",
    "        model=model_name,\n",
    "        max_model_len=5632,\n",
    "        max_num_seqs=5,\n",
    "        limit_mm_per_prompt=query_result.limit_mm_per_prompt,\n",
    "        seed=args.seed,\n",
    "        # 使用 modelscope \n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # We set temperature to 0.2 so that outputs can be different\n",
    "    # even when all prompts are identical when running batch inference.\n",
    "    sampling_params = SamplingParams(temperature=0.2, max_tokens=64)\n",
    "\n",
    "    outputs = llm.generate(query_result.inputs, sampling_params=sampling_params)\n",
    "\n",
    "    for o in outputs:\n",
    "        generated_text = o.outputs[0].text\n",
    "        print(generated_text)\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = FlexibleArgumentParser(\n",
    "        description=\"Demo on using vLLM for offline inference with \"\n",
    "        \"audio language models\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--query-type\",\n",
    "        \"-q\",\n",
    "        type=str,\n",
    "        default=\"mixed_modalities\",\n",
    "        choices=query_map.keys(),\n",
    "        help=\"Query type.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seed\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=\"Set the seed when initializing `vllm.LLM`.\",\n",
    "    )\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
