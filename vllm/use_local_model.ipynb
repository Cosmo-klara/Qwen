{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc75ff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CC=/usr/bin/gcc-9\n",
    "!export CXX=/usr/bin/g++-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97e9fd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export VLLM_USE_MODELSCOPE=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f8aa1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,7\"\n",
    "os.environ[\"NCCL_DEBUG\"] = \"WARN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648268a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-03 23:49:09 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-03 23:49:11 [config.py:2968] Downcasting torch.float32 to torch.bfloat16.\n",
      "INFO 12-03 23:49:22 [config.py:717] This model supports multiple tasks: {'generate', 'score', 'reward', 'classify', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 12-03 23:49:22 [config.py:1770] Defaulting to use mp for distributed inference\n",
      "INFO 12-03 23:49:22 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 12-03 23:49:22 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 12-03 23:49:24 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='../cache/modelscope/Qwen/Qwen2.5-Omni-3B', speculative_config=None, tokenizer='../cache/modelscope/Qwen/Qwen2.5-Omni-3B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=5632, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=../cache/modelscope/Qwen/Qwen2.5-Omni-3B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}\n",
      "WARNING 12-03 23:49:24 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 56 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 12-03 23:49:24 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 10485760, 10, 'psm_dce69926'), local_subscribe_addr='ipc:///tmp/5018790f-b1b2-42b6-a413-651ba3af0384', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 12-03 23:49:25 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fbb92204730>\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=30274)\u001b[0;0m INFO 12-03 23:49:25 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d513caa0'), local_subscribe_addr='ipc:///tmp/dcdec49f-4786-4142-a36e-adfff1f3a2a4', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 12-03 23:49:25 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fbb92204a30>\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=30273)\u001b[0;0m INFO 12-03 23:49:25 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_eacf95c5'), local_subscribe_addr='ipc:///tmp/48ce2cf4-96db-4c2e-a816-5c70ce45b9e7', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=30273)\u001b[0;0m INFO 12-03 23:49:26 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=30273)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=30274)\u001b[0;0m INFO 12-03 23:49:26 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 12-03 23:49:26 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=30274)\u001b[0;0m INFO 12-03 23:49:26 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "NCCL version 2.21.5+cuda11.0\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=30273)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=30274)\u001b[0;0m INFO 12-03 23:49:26 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/liuchi/.cache/vllm/gpu_p2p_access_cache_for_1,7.json\n",
      "INFO 12-03 23:49:26 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/liuchi/.cache/vllm/gpu_p2p_access_cache_for_1,7.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=30273)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=30274)\u001b[0;0m WARNING 12-03 23:49:26 [custom_all_reduce.py:146] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 12-03 23:49:26 [custom_all_reduce.py:146] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=30273)\u001b[0;0m INFO 12-03 23:49:26 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_36ee3e88'), local_subscribe_addr='ipc:///tmp/70b9e238-7ba6-4d50-ac6a-8bc9fbcdc230', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=30273)\u001b[0;0m INFO 12-03 23:49:27 [parallel_state.py:1004] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=30273)\u001b[0;0m INFO 12-03 23:49:27 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=30274)\u001b[0;0m INFO 12-03 23:49:27 [parallel_state.py:1004] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=30274)\u001b[0;0m INFO 12-03 23:49:27 [cuda.py:221] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=30273)\u001b[0;0m The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=30274)\u001b[0;0m The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=30274)\u001b[0;0m WARNING 12-03 23:49:35 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=30274)\u001b[0;0m INFO 12-03 23:49:35 [gpu_model_runner.py:1329] Starting to load model ../cache/modelscope/Qwen/Qwen2.5-Omni-3B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=30274)\u001b[0;0m You are attempting to use Flash Attention 2 without specifying a torch dtype. This might lead to unexpected behaviour\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=30273)\u001b[0;0m WARNING 12-03 23:49:35 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=30273)\u001b[0;0m INFO 12-03 23:49:35 [gpu_model_runner.py:1329] Starting to load model ../cache/modelscope/Qwen/Qwen2.5-Omni-3B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=30273)\u001b[0;0m You are attempting to use Flash Attention 2 without specifying a torch dtype. This might lead to unexpected behaviour\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=30274)\u001b[0;0m INFO 12-03 23:49:35 [config.py:3614] cudagraph sizes specified by model runner [] is overridden by config []\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=30273)\u001b[0;0m INFO 12-03 23:49:35 [config.py:3614] cudagraph sizes specified by model runner [] is overridden by config []\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec22ca32343f4e8d8709ae40c028e2a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=30274)\u001b[0;0m INFO 12-03 23:49:38 [loader.py:458] Loading weights took 3.23 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=30273)\u001b[0;0m INFO 12-03 23:49:39 [loader.py:458] Loading weights took 3.39 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=30274)\u001b[0;0m INFO 12-03 23:49:39 [gpu_model_runner.py:1347] Model loading took 5.0670 GiB and 3.570801 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=30273)\u001b[0;0m INFO 12-03 23:49:39 [gpu_model_runner.py:1347] Model loading took 5.0670 GiB and 3.732244 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=30274)\u001b[0;0m INFO 12-03 23:49:42 [gpu_model_runner.py:1620] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=30273)\u001b[0;0m INFO 12-03 23:49:42 [gpu_model_runner.py:1620] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.\n",
      "INFO 12-03 23:49:51 [kv_cache_utils.py:634] GPU KV cache size: 650,736 tokens\n",
      "INFO 12-03 23:49:51 [kv_cache_utils.py:637] Maximum concurrency for 5,632 tokens per request: 115.54x\n",
      "INFO 12-03 23:49:51 [kv_cache_utils.py:634] GPU KV cache size: 753,584 tokens\n",
      "INFO 12-03 23:49:51 [kv_cache_utils.py:637] Maximum concurrency for 5,632 tokens per request: 133.80x\n",
      "INFO 12-03 23:49:51 [core.py:159] init engine (profile, create kv cache, warmup model) took 11.91 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-03 23:50:00 [core_client.py:439] Core engine process 0 ready.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "model_name = \"../cache/modelscope/Qwen/Qwen2.5-Omni-3B\"\n",
    "from torch import bfloat16\n",
    "llm = LLM(\n",
    "    model=model_name,\n",
    "    # 使用 bfloat16 精度\n",
    "    dtype=bfloat16,\n",
    "    max_model_len=5632,\n",
    "    tensor_parallel_size=2,\n",
    "    max_num_seqs=5,\n",
    "    # gpu_memory_utilization=0.6,\n",
    "    # device = \"cuda:7\",\n",
    "\n",
    "    # nccl的问题，据说 nvidia-nccl-cu11 中的 nccl 基于 cu110 编译的，无法使用流功能\n",
    "    enforce_eager=True,\n",
    "    # 使用 modelscope \n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecf39b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
