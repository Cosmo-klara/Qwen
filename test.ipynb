{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47bec5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61e5b4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch版本: 2.6.0+cu118\n",
      "CUDA是否可用: True\n",
      "CUDA版本: 11.8\n",
      "当前GPU设备: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch版本: {torch.__version__}\")\n",
    "print(f\"CUDA是否可用: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA版本: {torch.version.cuda}\")\n",
    "    print(f\"当前GPU设备: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fac4bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "from modelscope import snapshot_download\n",
    "from modelscope import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor\n",
    "from qwen_omni_utils import process_mm_info\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a176c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: ./cache/modelscope/Qwen/Qwen2.5-Omni-3B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 15:01:50,092 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    }
   ],
   "source": [
    "# model_dir = snapshot_download(\n",
    "#     \"Qwen/Qwen2.5-Omni-7B\", \n",
    "#     cache_dir=\"./cache/modelscope\"\n",
    "# )\n",
    "model_dir = snapshot_download(\n",
    "    'Qwen/Qwen2.5-Omni-3B',\n",
    "    cache_dir=\"./cache/modelscope\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46674d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.46s/it]\n"
     ]
    }
   ],
   "source": [
    "model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n",
    "    model_dir,\n",
    "    device_map=\"cuda:1\",\n",
    "    dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "model.disable_talker()\n",
    "processor = Qwen2_5OmniProcessor.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fbd67dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型设备分布: {'': device(type='cuda', index=1)}\n"
     ]
    }
   ],
   "source": [
    "print(\"模型设备分布:\", model.hf_device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95fd5288",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuchi/anaconda3/envs/qwen_lora/lib/python3.10/site-packages/librosa/core/audio.py:172: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "qwen-vl-utils using torchvision to read video.\n"
     ]
    }
   ],
   "source": [
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"video\", \"video\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/draw.mp4\"},\n",
    "            {\"type\": \"text\", \"text\": \"Could you detail events during different time segments? Format strictly:\\nFrom xx to xx, event1.\\nFrom xx to xx, event2.\\n...\"}\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "USE_AUDIO_IN_VIDEO = True\n",
    "\n",
    "text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "audios, images, videos = process_mm_info(conversation, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n",
    "inputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=\"pt\", padding=True, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n",
    "inputs = inputs.to(model.device).to(model.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "003016b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['system\\nYou are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\\nuser\\nCould you detail events during different time segments? Format strictly:\\nFrom xx to xx, event1.\\nFrom xx to xx, event2.\\n...\\nassistant\\nFrom 0.00 to 1.00, the person is holding a tablet with a guitar drawing on it.\\nFrom 1.00 to 2.00, the person is using a pen to draw on the tablet.\\nFrom 2.00 to 3.00, the person is holding the tablet with one hand and using the pen with the other hand.\\nFrom 3.00 to 4.00, the person is using the pen to draw on the tablet.\\nFrom 4.00 to 5.00, the person is holding the tablet with one hand and using the pen with the other hand.\\nFrom 5.00 to 6.00, the person is using the pen to draw on the tablet.\\nFrom 6.00 to 7.00, the person is holding the tablet with one hand and using the pen with the other hand.\\nFrom 7.00 to 8.00, the person is using the pen to draw on the tablet.\\nFrom 8.00 to 9.00, the person is holding the tablet with one hand and using the pen with the other hand.']\n"
     ]
    }
   ],
   "source": [
    "text_ids = model.generate(**inputs, use_audio_in_video=USE_AUDIO_IN_VIDEO, return_audio=False)\n",
    "\n",
    "text = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08d90468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型设备分布: {'': device(type='cuda', index=1)}\n"
     ]
    }
   ],
   "source": [
    "print(\"模型设备分布:\", model.hf_device_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen_lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
